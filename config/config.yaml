act_fn: silu
architectures: JerryForCausalLM
cache_recent_size: 1024
cache_start_size: 10
cache_type: recent
word_embed_proj_dim: 256
hidden_size: 256
dropout: 0.0
dtype: float16
flash_attention: false
group_size_ratio: 0.25
load_in_4bit: -1
load_in_lowbit_groupsize: -1
load_in_quant_type: gptq
max_seq_len: 256
multiple_of: 32
n_heads: 8
n_kv_heads: 4
n_layers: 12
neftune_noise_alpha: 0.1
norm_eps: 1.0e-05
do_layer_norm_before: True
rope_beta: 10000.0
rope_scaling_factor: 1.0
rope_scaling_type: linear
use_bias: false
use_neftune: true
use_shift_short_attn: false
use_ssa_min_seq: 8192
vocab_file: ./tokenizer_model/chatglm_tokenizer/tokenizer.model
vocab_size: 64793
pad_token_id: 1
bos_token_id: 2
eos_token_id: 2