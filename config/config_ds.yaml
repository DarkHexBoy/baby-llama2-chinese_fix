dataset_params:
  max_seq_len: 1024
  train_data_path: [
        './data/pretrain_data.bin'
        #'./data/baidubaike_563w.bin',
        #'./data/medical_book.bin',
        # './data/medical_encyclopedia.bin',
        # './data/medical_qa.bin',
        # './data/wiki.bin'
    ]
  valid_data_path: [
        './data/valid_data.bin'
    ]
  sft_data_path: './data/sft_data.csv'
  test_data_path: [
        'data/test.json',
        'data/test_zh_0.json',
        'data/test_en_1.json',
    ]

model_path: 'best.model'
max_seq_len: 1024
model_params:
  dim: 1024
  n_layers: 28
  n_heads: 16        # 要能被dim整除
  n_kv_heads: 8      # 0及其以下,则取n_heads的值,为MHQ.为1则是MQA,大于1且小于n_layers则为GQA
  multiple_of: 32
  rope_scaling_factor: 1.0
  rope_scaling_type: 'linear'  # linear/dynamic
  dropout: 0.0
  norm_eps: 0.00001
  use_bias: False
  flash_attention: False
  dtype: 'float16'
  vocab_size: 64793
  vocab_file: './tokenizer_model/chatglm_tokenizer/tokenizer.model'
  model_type: 'Model'

train_params:
  use_deepspeed: True
  max_epoch: 3
  eval_interval: 1
  log_interval: 200
  eval_iters: 200
  eval_only: False
  always_save_ckpt: True
  init_from: 'scratch'
  grad_accum_steps: 64
  batch_size: 4
  learning_rate: 0.0003
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  decay_lr: True
  warmup_iters: 1000
  lr_decay_iters: 80000
  min_lr: 0.00001
  backend: 'nccl'
  device: 'cuda'
  compile: False

fine_tuning_params:
  ft_type: 'full_ft'  # full_ft/lora/qlora 
  lora_mudule: 'linear'  # linear/embedding/all
  lora_attn_dim: 8
  lora_attn_alpha: 128
  lora_dropout: 0.0
  lora_r_dropout: 0.0


eval_params:
  max_new_tokens: 100
  temperature: 1.0
  top_k: 30
  seed: 1337
  shot: 0